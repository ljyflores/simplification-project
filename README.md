# simplification-project

Welcome to the medical text simplification project repository! 
We aim to explore ways to train language models to simplify reports to make them more accessible to laypeople.
This repository contains the experiments presented in <a href=https://aclanthology.org/2023.findings-emnlp.322/>EMNLP 2023 Findings</a>.

Check out the demo through this <a href="https://huggingface.co/spaces/ljyflores/simplification-model-app">Streamlit app</a>!
## Set-up
To get started, clone this repository and set up a `simplification` environment as follows:
```
# Set up the environment
conda create --name simplification python=3.8
conda activate simplification
pip install -r requirements.txt
# Install SciSpacy models
pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_scibert-0.5.1.tar.gz
pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.1/en_core_web_lg-3.4.1-py3-none-any.whl
pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bionlp13cg_md-0.5.1.tar.gz
pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.3/en_core_sci_lg-0.5.3.tar.gz
```

We also set up a separate `simplification_questeval` environment to use QuestEval separately, as it conflicts with `simplification`.

```
# Set up the environment
conda create --name simplification_questeval python=3.9
conda activate simplification_questeval
pip install -r requirements_questeval.txt 
```

<b>OpenAI: </b> We also use the OpenAI API for various generation and evaluation tasks. To save your key (and use the evaluation script), first create a file called `openai_key` (no `.txt` extension!), then paste your key there! The script will look for this file and read the key from it. This key is also automatically excluded using the `.gitignore`, so we don't push our keys here online.

<b>Weights and Biases:</b> Before training, be sure to log in to <a href="https://wandb.ai/">wandb</a> (weights and biases), which helps us log experiments and keep track of their performance. To do this, set up a (free) account with wandb, then copy the API key! Back in the terminal, we can log in as follows:
```
wandb login <API_KEY>
```

## Data
Each dataset is stored using `json` files – each with two versions: `<dataset>.json` and `<dataset>_multiple.json`. 
* `<dataset>.json` is used for training
  * one example input is mapped to exactly one training label – so if there are 10 labels written for one input, the `<dataset>.json` will have 10 examples for this input
* `<dataset>_multiple.json` is used for evaluation
  *  one example input is mapped to all the training labels written for it – so if there are 10 labels written for one input, the `<dataset>_multiple.json` will still only have 1 example for it

```
{
  "train": [
             {"input": <str>,
             "labels": [<str>, <str>, ..., <str>]},
             {"input": <str>,
             "labels": [<str>, <str>, ..., <str>]},
             ...,
             {"input": <str>,
             "labels": [<str>, <str>, ..., <str>]},
            ],
  "test":  [
             {"input": <str>,
             "labels": [<str>, <str>, ..., <str>]},
             {"input": <str>,
             "labels": [<str>, <str>, ..., <str>]},
             ...,
             {"input": <str>,
             "labels": [<str>, <str>, ..., <str>]},
            ],
}
```

## Training

We've set up a script that reads in dataset from the `data` folder and trains a model with the specified parameters.
It then outputs a textfile of the summaries generated by the model for the `test` set, and places them in the `output` folder as `<dataset_name>.txt`.
Here's the structure of the command to train a model:
```
CUDA_VISIBLE_DEVICES=<gpu_id> python train.py --dataset <dataset> --model <model> --loss_type <loss_type> --lr <lr> --epochs <num_epochs> --batch_size <batch_size> --gradient_accumulation_steps <grad_acc> 
```

For example,
```
CUDA_VISIBLE_DEVICES=7 python train.py --dataset asset --lr 5e-4 --epochs 10 --batch_size 8 --gradient_accumulation_steps 8 --model bart_xsum
```
To use unlikelihood loss, add the training parameter `--loss_type ul`, which will automatically add the readability penalty. To further add the consistency penalty, use either `ul_inp`, `ul_lab`, or `ul_inp_lab`, which will penalize the model for generating entities that are not present in either the input, label, or both.

## Decoding
To run the decoding script, we require a model checkpoint and a dataset to use. Parameters like the intervals at which BERTScore and FK are calculated and the number of beams can be set in the script.

```
CUDA_VISIBLE_DEVICES=<gpu_id> python decode.py --dataset <dataset> --model <model> --checkpoint <checkpoint>
```

## Evaluation

To run the evaluation script that compares the output `.txt` file to the reference summaries in the `.json` file, run this command:
```
CUDA_VISIBLE_DEVICES=<gpu_id> python eval.py --dataset <dataset> --preds_path <model predictions>
```

## Citing

If you found our work useful, kindly cite it for more people to learn about it! Check out our <a href=https://aclanthology.org/2023.findings-emnlp.322/>paper</a> too!
```
Lorenzo Jaime Yu Flores, Heyuan Huang, Kejian Shi, Sophie Chheang, and Arman Cohan. 2023.
Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding.
In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 4859–4873, Singapore.
Association for Computational Linguistics.
```
